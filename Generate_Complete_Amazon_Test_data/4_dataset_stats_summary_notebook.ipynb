{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "from multiprocessing import Pool, Manager\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Data_Stats_Processor:\n",
    "    def __init__(self) -> None:\n",
    "        known_writer_id_set, unknown_writer_id_set, writer_id_to_index_mapping_dict, writer_index_to_id_mapping_dict = self.load_writer_info()\n",
    "        self.known_writer_id_set = known_writer_id_set\n",
    "        self.unknown_writer_id_set = unknown_writer_id_set\n",
    "        self.writer_id_to_index_mapping_dict = writer_id_to_index_mapping_dict\n",
    "        self.writer_index_to_id_mappping_dict = writer_index_to_id_mapping_dict\n",
    "        pass\n",
    "\n",
    "    def output_writer_num_to_id_mapping_dict(self):\n",
    "        id_to_num_mapping_file_path = \"./output/writer_100_review_40/writer_id_to_num_mapping_dict.json\"\n",
    "\n",
    "        with open(id_to_num_mapping_file_path, mode=\"r\") as fin:\n",
    "            id_to_num_dict = json.load(fin)\n",
    "        # endwith\n",
    "\n",
    "        num_to_id_dict = {}\n",
    "        for id, index in id_to_num_dict.items():\n",
    "            num_to_id_dict[int(index)] = id\n",
    "        # endfor\n",
    "\n",
    "        # ######## output #########\n",
    "        output_file_path = \"./output/writer_100_review_40/writer_num_to_id_mapping_dict.json\"\n",
    "        with open(output_file_path, mode=\"w\") as fout:\n",
    "            json.dump(num_to_id_dict, fout)\n",
    "        # endwith\n",
    "        pass\n",
    "\n",
    "    def load_writer_info(self):\n",
    "        # (1)\n",
    "        known_writer_id_file = \"./output/writer_100_review_40/known_writer_id_list.json\"\n",
    "        with open(known_writer_id_file, mode=\"r\") as fin:\n",
    "            known_writer_id_set = json.load(fin)\n",
    "            known_writer_id_set = set(known_writer_id_set)\n",
    "        # endwith\n",
    "\n",
    "        # (2)\n",
    "        unknown_writer_id_file = \"./output/writer_100_review_40/unknown_writer_id_list.json\"\n",
    "        with open(unknown_writer_id_file, mode=\"r\") as fin:\n",
    "            unknown_writer_id_set = json.load(fin)\n",
    "            unknown_writer_id_set = set(unknown_writer_id_set)\n",
    "        # endwith\n",
    "\n",
    "        # !! Include shipping writer here\n",
    "        # --------------------------- load unknown writer_id -----------------------\n",
    "        # all the unknown writer are in the shipping review dataset\n",
    "        shipping_reviewer_id_stats_file = \"./shipping_review_input/shipping_reviewer_stats.txt\"\n",
    "        # TODO: when generate final dataset, here it uses set, not list, so that order is lost\n",
    "        # TODO: anyway, any writer id better than 310 (mapping dict start from 1), start from 311 are unknown writers.\n",
    "        shipping_unknown_writer_id_list = []\n",
    "        with open(shipping_reviewer_id_stats_file, mode=\"r\") as fin:\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                parts = line.split()\n",
    "                shipping_unknown_writer_id_list.append(parts[0])\n",
    "            # endfor\n",
    "        # endwith\n",
    "        # update unknown writer\n",
    "        unknown_writer_id_set.update(shipping_unknown_writer_id_list)\n",
    "\n",
    "\n",
    "        # (3)\n",
    "        id_to_num_mapping_file_path = \"./output/writer_100_review_40/writer_id_to_num_mapping_dict.json\"\n",
    "        with open(id_to_num_mapping_file_path, mode=\"r\") as fin:\n",
    "            writer_id_to_index_mapping_dict = json.load(fin)\n",
    "        # endwith\n",
    "        for shipping_writer_id in shipping_unknown_writer_id_list:\n",
    "            writer_id_to_index_mapping_dict[shipping_writer_id] = len(writer_id_to_index_mapping_dict) + 1\n",
    "        #endfor\n",
    "\n",
    "        # (4)\n",
    "        writer_index_to_id_mapping_dict = {}\n",
    "        for k, v in writer_id_to_index_mapping_dict.items():\n",
    "            writer_index_to_id_mapping_dict[v] = k\n",
    "        #endfor\n",
    "\n",
    "        return known_writer_id_set, unknown_writer_id_set, writer_id_to_index_mapping_dict, writer_index_to_id_mapping_dict\n",
    "\n",
    "    def single_worker_get_writer_stats(self, file_path_list):\n",
    "\n",
    "        known_writer_sample_num_dict = defaultdict(int)\n",
    "        unknown_writer_sample_num_dict = defaultdict(int)\n",
    "\n",
    "        known_writer_doc_list_dict = defaultdict(set)\n",
    "        unknown_writer_doc_list_dict = defaultdict(set)\n",
    "\n",
    "        # text length stats\n",
    "        text_length_list = []\n",
    "\n",
    "        # unique combination of writer, product category, sentiment, review types in train, non-novel-test and novel-test\n",
    "        known_writer_text_pair_set = set()\n",
    "        unknown_writer_text_pair_set = set()\n",
    "\n",
    "        for file_path in tqdm(file_path_list, desc=\"all files\"):\n",
    "            with open(file_path, mode=\"r\", encoding=\"utf16\") as fin:\n",
    "                csv_reader = csv.DictReader(fin, delimiter=\",\", quotechar=\"|\")\n",
    "                # instanceid,text,reported_writer_id,real_writer_id,sentiment,product,novelty_indicator,novel_instance,text_id\n",
    "                for row in csv_reader:\n",
    "                    text = row[\"text\"]\n",
    "\n",
    "                    # text length stats\n",
    "                    text_len = len(text.strip().split())\n",
    "                    text_length_list.append(text_len)\n",
    "\n",
    "                    real_writer_id = int(row[\"real_writer_id\"])\n",
    "                    real_writer_str = self.writer_index_to_id_mappping_dict[real_writer_id]\n",
    "\n",
    "                    assert real_writer_str in self.known_writer_id_set or real_writer_str in self.unknown_writer_id_set\n",
    "\n",
    "                    if real_writer_str in self.known_writer_id_set:\n",
    "                        known_writer_sample_num_dict[real_writer_str] += 1\n",
    "                        # add document set\n",
    "                        known_writer_doc_list_dict[real_writer_str].add(file_path)\n",
    "                        # writer text pair\n",
    "                        known_writer_text_pair_set.add((real_writer_str, text))\n",
    "                    # endif\n",
    "\n",
    "                    if real_writer_str in self.unknown_writer_id_set:\n",
    "                        unknown_writer_sample_num_dict[real_writer_str] += 1\n",
    "                        # add document set\n",
    "                        unknown_writer_doc_list_dict[real_writer_str].add(file_path)\n",
    "                        # writer text pair\n",
    "                        unknown_writer_text_pair_set.add((real_writer_str, text))\n",
    "                    # endif\n",
    "                # endfor\n",
    "            # endwith\n",
    "        # endfor\n",
    "\n",
    "        # get document frequency dict\n",
    "        known_writer_df_dict = {}\n",
    "        for k, v in known_writer_doc_list_dict.items():\n",
    "            known_writer_df_dict[k] = len(v)\n",
    "        #endif\n",
    "\n",
    "        unknown_writer_df_dict = {}\n",
    "        for k, v in unknown_writer_doc_list_dict.items():\n",
    "            unknown_writer_df_dict[k] = len(v)\n",
    "        #endif\n",
    "\n",
    "\n",
    "        return text_length_list, known_writer_sample_num_dict, unknown_writer_sample_num_dict, known_writer_df_dict, unknown_writer_df_dict, known_writer_text_pair_set, unknown_writer_text_pair_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 864 csv files.\n"
     ]
    }
   ],
   "source": [
    "folder = \"./output/writer_100_review_40/NLT_complete_trials_Nov_9_2021/OND/NLT\"\n",
    "all_file_path_list = []\n",
    "for root, subdir, file_list in os.walk(folder):\n",
    "    for file_name in file_list:\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            all_file_path_list.append(file_path)\n",
    "        # endif\n",
    "    # endfor\n",
    "# endwith\n",
    "print(f\"There are {len(all_file_path_list)} csv files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all files: 100%|██████████| 864/864 [00:33<00:00, 25.95it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = Data_Stats_Processor()\n",
    "\n",
    "# ###### single ######\n",
    "\n",
    "text_length_list, known_writer_sample_num_dict, unknown_writer_sample_num_dict, \\\n",
    "known_writer_df_dict, unknown_writer_df_dict, \\\n",
    "known_writer_text_pair_set, unknown_writer_text_pair_set = processor.single_worker_get_writer_stats(all_file_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text len mean: 218.1146556712963\n",
      "text len min: 1\n",
      "text len max: 4683\n"
     ]
    }
   ],
   "source": [
    "# text stats\n",
    "text_length_arr = np.array(text_length_list)\n",
    "print(f\"text len mean: {text_length_arr.mean()}\")\n",
    "print(f\"text len min: {np.amin(text_length_arr)}\")\n",
    "print(f\"text len max: {np.amax(text_length_arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data unique entries known writer: 3995\n",
      "Test data unique entries unknown writer: 14576\n"
     ]
    }
   ],
   "source": [
    "# unique combinations of writer, product category, sentiment, review types did we actual use in training, non-novel test and novel-test\n",
    "# using (writer_id, text) is enough\n",
    "print(f\"Test data unique entries known writer: {len(known_writer_text_pair_set)}\")\n",
    "print(f\"Test data unique entries unknown writer: {len(unknown_writer_text_pair_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "normalized_known_writer_sample_num_dict = {k: (v * 1.0 / known_writer_df_dict[k]) for k, v in known_writer_sample_num_dict.items()}\n",
    "normalized_unknown_writer_sample_num_dict = {k: (v * 1.0 / unknown_writer_df_dict[k]) for k, v in unknown_writer_sample_num_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unknown writer: 934\n",
      "################## NON-normalized ###########\n",
      "---------------------\n",
      "Test dataset KNOWN writer:\n",
      "mean: 5972.16\n",
      "min: 998\n",
      "max: 27764\n",
      "---------------------\n",
      "Test dataset UNKNOWN writer:\n",
      "mean: 470.64668094218416\n",
      "min: 83\n",
      "max: 3630\n",
      "################## normalized ################\n",
      "---------------------\n",
      "Test dataset KNOWN writer:\n",
      "mean: 7.01575254696484\n",
      "min: 1.476858345021038\n",
      "max: 32.13425925925926\n",
      "---------------------\n",
      "Test dataset UNKNOWN writer:\n",
      "mean: 1.6390561618072932\n",
      "min: 1.0\n",
      "max: 7.546777546777546\n"
     ]
    }
   ],
   "source": [
    "test_data_known_writer_sample_num_arr = np.array(list(known_writer_sample_num_dict.values()))\n",
    "test_data_unknown_writer_sample_num_arr = np.array(list(unknown_writer_sample_num_dict.values()))\n",
    "\n",
    "normalized_test_data_known_writer_sample_num_arr = np.array(list(normalized_known_writer_sample_num_dict.values()))\n",
    "normalized_test_data_unknown_writer_sample_num_arr = np.array(list(normalized_unknown_writer_sample_num_dict.values()))\n",
    "\n",
    "print(f\"Total unknown writer: {len(processor.unknown_writer_id_set)}\")\n",
    "\n",
    "\n",
    "print(\"################## NON-normalized ###########\")\n",
    "print(\"---------------------\")\n",
    "print(\"Test dataset KNOWN writer:\")\n",
    "print(f\"mean: {np.mean(test_data_known_writer_sample_num_arr)}\")\n",
    "print(f\"min: {np.amin(test_data_known_writer_sample_num_arr)}\")\n",
    "print(f\"max: {np.amax(test_data_known_writer_sample_num_arr)}\")\n",
    "print(\"---------------------\")\n",
    "print(f\"Test dataset UNKNOWN writer:\")\n",
    "print(f\"mean: {np.mean(test_data_unknown_writer_sample_num_arr)}\")\n",
    "print(f\"min: {np.amin(test_data_unknown_writer_sample_num_arr)}\")\n",
    "print(f\"max: {np.amax(test_data_unknown_writer_sample_num_arr)}\")\n",
    "\n",
    "\n",
    "print(\"################## normalized ################\")\n",
    "print(\"---------------------\")\n",
    "print(\"Test dataset KNOWN writer:\")\n",
    "print(f\"mean: {np.mean(normalized_test_data_known_writer_sample_num_arr)}\")\n",
    "print(f\"min: {np.amin(normalized_test_data_known_writer_sample_num_arr)}\")\n",
    "print(f\"max: {np.amax(normalized_test_data_known_writer_sample_num_arr)}\")\n",
    "print(\"---------------------\")\n",
    "print(f\"Test dataset UNKNOWN writer:\")\n",
    "print(f\"mean: {np.mean(normalized_test_data_unknown_writer_sample_num_arr)}\")\n",
    "print(f\"min: {np.amin(normalized_test_data_unknown_writer_sample_num_arr)}\")\n",
    "print(f\"max: {np.amax(normalized_test_data_unknown_writer_sample_num_arr)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3163820cb05a1638c106be412376f4b22fef14153e262bbca2cf1c968bd27db8"
  },
  "kernelspec": {
   "display_name": "gat_reason",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}